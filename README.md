# GCP Data Archival & Retrieval Platform

**Compliance-First Data Engineering Architecture on Google Cloud**

This project implements an **enterprise-grade Archival, Compliance & Analytics Data Platform** on **Google Cloud Platform (GCP)**.  
It ingests structured business data (CSV) and unstructured physical evidence (PDF invoices), dynamically links them, and enforces strict governance rules before promoting data to analytics layers.

Unlike traditional ETL pipelines, this system follows a **Gatekeeper Pattern**:  
> **Data is only promoted if physical proof exists.**

---

## ğŸ“Œ Core Business Rule
> **â€œNo Order is valid without physical proof (Invoice PDF).â€**

Only orders with verified invoice PDFs are promoted to the **Golden Record (Operational Curated Layer)** and then to the **Business Analytics Layer**.

This guarantees:
* âœ… Regulatory compliance
* âœ… Zero orphan records in analytics
* âœ… Full auditability
* âœ… Late-arrival recovery

---

## ğŸ§© Key Features
* âœ… **Hybrid Ingestion (CSV + PDF)**
* ğŸ›¡ï¸ **Gatekeeper Architecture** (hard compliance boundary)
* ğŸ—‚ï¸ **Zoned Data Lake** (Landing â†’ Processed â†’ Archive â†’ Error)
* ğŸ§¾ **Physical proof enforcement**
* ğŸ” **Late-arrival recovery** (Housekeeping DAG)
* ğŸ“Š **BI-ready datasets**
* ğŸ” **IAM Service Account security**
* ğŸ§® **Full audit trail** (`data_error_logs`)
* âš™ï¸ **Apache Airflow orchestration** (5 DAGs)

---

## ğŸ—ï¸ System Architecture

### 1. Storage Layer â€“ Google Cloud Storage (Data Lake)

| Zone | Path | Description |
| :--- | :--- | :--- |
| **Landing** | `landing/structured/` | Incoming CSV files |
| | `landing/unstructured/` | Incoming invoice PDFs |
| **Processed** | `processed/` | Archived structured files |
| **Archive** | `archive/pdfs/` | Validated invoice PDFs |
| **Error** | `error/pdfs/` | Orphan or invalid PDFs |

**Benefits:**
* Clear lifecycle management
* Audit-safe (no deletes, only moves)
* Physical isolation of bad data

---

### 2. Data Warehouse â€“ BigQuery

#### A. Raw Dataset (`raw_dataset`)
*Purpose: Staging layer (loose schema).*

**Tables:** `customers_raw`, `orders_raw`, `order_items_raw`,  
`payments_raw`, `returns_raw`, `attachments_raw`, `pdf_manifest_raw`

**Metadata Columns:** * `row_status` (default = `FAIL`)
* `ingestion_time`

#### B. Curated Dataset (`curated_dataset`)
*Purpose: Golden Record layer (strict schema & referential integrity).*

**Tables:** `customers_curated`, `orders_curated`, `order_items_curated`,  
`payments_curated`, `returns_curated`, `attachments_curated`, `pdf_manifest_curated`

**Audit Table:** `data_error_logs`

* **Tracks:** Compliance failures, Orphan PDFs, Invalid PDFs, Recovery events
* **Includes:** `resolved_flag`, `resolved_at`

#### C. Business Curated Dataset (`business_curated_dataset`)
*Purpose: Analytics & BI.*

**Tables:**
* `customer_lifetime_value`
* `customer_segments`
* `monthly_sales`
* `product_performance`
* `product_returns`

**Guarantee:** Analytics only use compliant data.

---

## ğŸ”„ Pipeline Orchestration (Apache Airflow â€“ 5 DAGs)

The platform uses a **Sequential Dependency Architecture**.

### DAG 1 â€” Ingest & Link
**Goal:** Ingest raw data and physically validate PDFs.

1.  Load CSVs â†’ Raw tables
2.  Normalize metadata (`row_status`, `ingestion_time`)
3.  Process PDFs in parallel:
    * **Valid** â†’ `archive/pdfs/` + `attachments_raw`
    * **Orphan** â†’ `error/pdfs/` + `data_error_logs`
    * **Invalid** â†’ `error/pdfs/` + `data_error_logs`

### DAG 2 â€” Governance & Audit
**Goal:** Certification layer (Gatekeeper).

* **SQL-only**
* Sets `row_status = PASS` only if attachment exists
* Orders without PDFs remain `FAIL`

*Creates a hard compliance boundary.*

### DAG 3 â€” Operational Curation (Golden Records)
**Goal:** Create trusted datasets.

* Filters only `row_status = PASS` orders
* Cascades filtering to: `order_items`, `payments`, `returns`, `attachments`

*Guarantee: Zero orphan records.*

### DAG 4 â€” Business Analytics Curation
**Goal:** BI-ready datasets.

* **Creates:** Customer Lifetime Value, Customer Segments, Monthly Sales, Product Performance, Product Returns

### DAG 5 â€” Housekeeping & Remediation
**Goal:** Recover late-arriving invoices.

*Runs independently of ingestion.*

**Logic:**
1.  Scans `landing/unstructured/` for new PDFs
2.  Matches against unresolved compliance failures
3.  **If found:**
    * Moves PDF â†’ `archive/pdfs/`
    * Inserts into `attachments_raw`
    * Updates `data_error_logs` (`resolved_flag = true`, `resolved_at = timestamp`)

**Enables:**
* Late invoice recovery
* Error lifecycle tracking
* Regulatory audit history

*Governance (DAG 2) is rerun intentionally after remediation.*

---

## ğŸ” Security & Authentication

Uses a dedicated **IAM Service Account** (no user credentials).

* **Service Account:** `airflow-sa@archive-demo-project-484906.iam.gserviceaccount.com`
* **Authentication:** `GOOGLE_APPLICATION_CREDENTIALS`
* **IAM Roles:**
    * BigQuery Data Editor
    * BigQuery Job User
    * Storage Object Admin

**Benefits:** Least privilege, Auditable, Portable, Production-ready.

---

## ğŸ§ª Technologies Used

| Tool | Purpose |
| :--- | :--- |
| **Google Cloud Storage** | Data Lake |
| **BigQuery** | Enterprise Data Warehouse |
| **Apache Airflow** | Orchestration |
| **Python** | PDF parsing, concurrency, recovery logic |
| **SQL (Standard)** | Governance & transformations |
| **Docker** | Local Airflow runtime |

---

## ğŸ“Š Business Impact

1.  **Regulatory compliance** â€“ every order has physical proof
2.  **Golden records** â€“ zero orphan analytics
3.  **Trusted BI** â€“ accurate KPIs
4.  **Operational excellence** â€“ retryable, auditable, recoverable
5.  **Error lifecycle management** â€“ unresolved â†’ resolved tracking

*This platform transforms raw, chaotic data into a governed enterprise data asset.*

---

## ğŸ“ Project Structure

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_1_ingestion.py
â”‚   â”œâ”€â”€ dag_2_governance.py
â”‚   â”œâ”€â”€ dag_3_curation.py
â”‚   â”œâ”€â”€ dag_4_business_analytics.py
â”‚   â””â”€â”€ dag_5_housekeeping.py
â””â”€â”€ sql/
    â”œâ”€â”€ curated_ddls.sql
    â””â”€â”€ business_ddls.sql

docker-compose.yml
README.md
```

## ğŸš€ Future Enhancements

1. **SCD Type 2:** Implement Slowly Changing Dimensions for Customer history.
2. **Data Quality:** Integrate Great Expectations for schema validation.
3. **BI Integration:** Connect datasets to Looker or Power BI.
4. **CDC:** Implement Change Data Capture for real-time ingestion.
5. **Alerting:** Configure Slack/Email alerts for Orphan PDF detection.
