# GCP Data Archival & Retrieval Platform

**Compliance-Driven Data Engineering Pipeline**

This project implements an enterprise-grade data archival, governance, and analytics platform on **Google Cloud Platform (GCP)**. It ingests structured business data (CSV) and unstructured evidence (PDF invoices), links them dynamically, and enforces strict compliance rules before promoting data to analytics layers.

---

## ğŸ“Œ Business Rule
> **"No Order is valid without physical proof (Invoice PDF)."**

Only orders with verified invoice PDFs are promoted to the **Golden Record (Curated Layer)**.

---

## ğŸ§© Key Features
* âœ… **Hybrid Ingestion:** Handles both Structured (CSV) and Unstructured (PDF) data types.
* ğŸ›¡ï¸ **Gatekeeper Architecture:** Enforces compliance by quarantining data without physical proof.
* ğŸ—‚ï¸ **Zoned Data Lake:** Organized into `Landing`, `Processed`, `Archive`, and `Error` zones.
* ğŸ“Š **Analytics-Ready:** Produces clean, curated datasets for BI consumption.
* ğŸ” **Production Security:** Uses IAM Service Accounts (Least Privilege) instead of personal credentials.
* ğŸ” **Orchestration:** Fully automated using **Apache Airflow** (4 Sequential DAGs).
* ğŸ“ˆ **Business Insights:** Derives CLV, segmentation, and product performance metrics.

---

## ğŸ—ï¸ Architecture Overview

### 1. Storage Layer (Google Cloud Storage)
The Data Lake is zoned for lifecycle management:
* `landing/structured/` â†’ Raw CSV files (Orders, Customers, etc.)
* `landing/unstructured/` â†’ Raw PDF invoices
* `processed/` â†’ Archived/Processed CSVs
* `archive/pdfs/` â†’ **Validated** Evidence (Linked to Orders)
* `error/pdfs/` â†’ **Orphan** or Invalid PDFs (Quarantine)

### 2. Compute & Warehouse (BigQuery)
The data warehouse is stratified into three layers:

#### **A. Raw Dataset (`raw_dataset`)**
*Staging layer with loose schema (Strings).*
* **Tables:** `customers_raw`, `orders_raw`, `order_items_raw`, `payments_raw`, `returns_raw`, `attachments_raw`, `pdf_manifest_raw`
* **Audit Columns:** `row_status` (Default: `'FAIL'`), `ingestion_time`

#### **B. Curated Dataset (`curated_dataset`)**
*Golden records with strict schema (Types) and referential integrity.*
* **Tables:** `customers_curated`, `orders_curated`, `order_items_curated`, `payments_curated`, `returns_curated`, `attachments_curated`, `pdf_manifest_curated`
* **Audit Log:** `data_error_logs` (Centralized error tracking)

#### **C. Business Curated Dataset (`business_curated_dataset`)**
*Aggregated tables for reporting.*
* `customer_lifetime_value`
* `customer_segments`
* `monthly_sales`
* `product_performance`
* `product_returns`

---

## ğŸ”„ Pipeline Orchestration (Airflow)

The system is driven by **4 Sequential DAGs**:

### ğŸŸ¢ DAG 1 â€” Ingest & Link
**Goal:** Load raw data and physically validate PDFs.
1.  **Structured:** Load CSVs into BigQuery Raw tables.
    * Normalize: Set `row_status = 'FAIL'`, stamp `ingestion_time`.
    * Move processed CSVs to `processed/`.
2.  **Unstructured:** Scan PDF invoices & extract `order_id` from filename.
    * **Match:** Move to `archive/` â†’ Insert into `attachments_raw`.
    * **Orphan:** Move to `error/` â†’ Log in `data_error_logs`.

### ğŸŸ¡ DAG 2 â€” Governance & Audit
**Goal:** Certify valid orders using the Gatekeeper logic.
* **Logic:** Updates `row_status` to `'PASS'` **only** if a valid attachment exists.
    ```sql
    UPDATE orders_raw
    SET row_status = 'PASS'
    WHERE order_id IN (SELECT order_id FROM attachments_raw);
    ```
* *Result:* All unverified orders remain as `'FAIL'`.

### ğŸ”µ DAG 3 â€” Curation (Golden Records)
**Goal:** Produce strict, analytics-ready datasets.
* **Customers:** Fully curated.
* **Orders:** Only orders with `row_status = 'PASS'`.
* **Child Tables (Items, Payments, Returns):**
    * Filtered via: `WHERE order_id IN (SELECT order_id FROM orders_curated)`
* **Error Handling:** Failed orders are logged to `data_error_logs` to prevent reporting gaps.

### ğŸŸ£ DAG 4 â€” Business Analytics
**Goal:** Create business insight tables for dashboards.

| Table Name | Description |
| :--- | :--- |
| `customer_lifetime_value` | Total revenue per customer |
| `customer_segments` | High / Medium / Low value tiering |
| `monthly_sales` | Monthly revenue & order volume trends |
| `product_performance` | Revenue by product |
| `product_returns` | Return rates by product |

---

## ğŸ” Security Model

We migrated from personal credentials to a **Service Account** architecture for production safety.

### **Authentication Strategy**
* âŒ **Original (Dev):** `gcloud auth application-default login` (Personal User)
* âœ… **Current (Prod):** IAM Service Account

### **Service Account Details**
* **Account:** `airflow-sa@archive-demo-project-484906.iam.gserviceaccount.com`
* **Configuration:** `GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/credentials/airflow-sa-key.json`

### **IAM Roles (Least Privilege)**
* `BigQuery Data Editor` (Read/Write Tables)
* `BigQuery Job User` (Run Queries)
* `Storage Object Admin` (Move/Rename Files)
* `Storage Bucket Viewer` (List Files)

---

## ğŸ§ª Technologies Used

| Tool | Purpose |
| :--- | :--- |
| **Google Cloud Storage** | Data Lake (Landing, Archive) |
| **BigQuery** | Enterprise Data Warehouse |
| **Apache Airflow** | Workflow Orchestration & Scheduling |
| **Python** | Custom Pipeline Logic & GCS Operations |
| **SQL (Standard)** | Data Transformations & Business Logic |
| **Docker** | Local Airflow Runtime |

---

## ğŸ“Š Business Value

1.  **Regulatory Compliance:** Ensures 100% of reported revenue is backed by physical evidence.
2.  **Data Integrity:** Prevents "Ghost Orders" (orphans) from corrupting financial reports.
3.  **Analytics-Ready:** Provides clean, type-safe datasets for immediate BI consumption.
4.  **Actionable Insights:** Enables deep analysis of Customer Segments, Sales Trends, and Product Returns.

---

## ğŸ“ Project Structure
```
airflow/
 â”œâ”€â”€ dags/
 â”‚   â”œâ”€â”€ dag_1_ingestion.py        # Ingest CSVs & Process PDFs
 â”‚   â”œâ”€â”€ dag_2_governance.py       # Gatekeeper Logic (Pass/Fail)
 â”‚   â”œâ”€â”€ dag_3_curation.py         # Create Golden Records
 â”‚   â””â”€â”€ dag_4_business_analytics.py # KPI Tables
 â””â”€â”€ sql/
     â”œâ”€â”€ curated_ddls.sql          # Table Definitions (Curated)
     â””â”€â”€ business_ddls.sql         # Table Definitions (Business)

docker-compose.yml                 # Airflow Container Config
README.md                          # Project Documentation
```

## ğŸš€ Future Enhancements

1. **SCD Type 2:** Implement Slowly Changing Dimensions for Customer history.
2. **Data Quality:** Integrate Great Expectations for schema validation.
3. **BI Integration:** Connect datasets to Looker or Power BI.
4. **CDC:** Implement Change Data Capture for real-time ingestion.
5. **Alerting:** Configure Slack/Email alerts for Orphan PDF detection.
